{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ./model/custom_model and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([1205, 1024]) in the checkpoint and torch.Size([1582, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([1205]) in the checkpoint and torch.Size([1582]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da27ab52d285439fac2ad2ecbbf5282a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1672aeeeb2894c8aa6607fc1555a625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\jjw28\\anaconda3\\envs\\for_gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8ed52508e142de82a8b10242a8432f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjw28\\anaconda3\\envs\\for_gpu\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 33.5115, 'train_samples_per_second': 8.952, 'train_steps_per_second': 0.895, 'train_loss': 60.48264973958333, 'epoch': 30.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=60.48264973958333, metrics={'train_runtime': 33.5115, 'train_samples_per_second': 8.952, 'train_steps_per_second': 0.895, 'train_loss': 60.48264973958333, 'epoch': 30.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer\n",
    "from utils.DataCollatorCTCWithPadding import DataCollatorCTCWithPadding\n",
    "import IPython.display as ipd\n",
    "import nlptutti as metrics\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "# from utils.compute_metrics import compute_metrics\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./model/custom_processor\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained('./model/custom_model',\n",
    "                                       pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                                       ignore_mismatched_sizes=True)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "data = load_dataset('./data/pre_datasets')\n",
    "subset_size = 10\n",
    "subset_train = data['train'].select([i for i in range(subset_size)])\n",
    "subset_train\n",
    "\n",
    "test_size = 10\n",
    "subset_test = data['test'].select([i for i in range(test_size)])\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./model/trained_model',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=4,\n",
    "  gradient_accumulation_steps=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500, \n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    "  dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "import nlptutti as metrics\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wer_metric = 0\n",
    "    cer_metric = 0\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    print(pred_str, label_str)\n",
    "    \n",
    "    for i in range(len(pred_str)):\n",
    "        preds = pred_str[i].replace(\" \", \"\")\n",
    "        labels = label_str[i].replace(\" \", \"\")\n",
    "        wer = metrics.get_wer(pred_str[i], label_str[i])['wer']\n",
    "        cer = metrics.get_cer(preds, labels)['cer']\n",
    "        wer_metric += wer\n",
    "        cer_metric += cer\n",
    "        \n",
    "    wer_metric = wer_metric/len(pred_str)\n",
    "    cer_metric = cer_metric/len(pred_str)\n",
    "    \n",
    "    return {\"wer\": wer_metric, \"cer\": cer_metric}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=subset_train,\n",
    "    eval_dataset=subset_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb89a011520a4123a6579cffc2cc7c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['뀓증뀓증뀓증뀓증뀓증뀓증뀓증뀓증쉽탐쉽증해뙤증겡퉁뽜증댄증셍증룹멀놓증찌펭팝증헏롬왐초뀓룹뽄룹넌웓앤흉냑궐냑뀓퉁뀓퉁뀓증당증겨증혐꿕겨빵섣갈증재뀓뵹요핍꼽뀓빨텔금뀓증울겨대탑겨왐뽜뿍룹멀룹뀓썬뀓멀뜽슴틴흉금흉멀흉증츄증덱뀓증덱증덱증옏뀓옏증튤뤙멀뤙옏뤙멀튤귝기뤙기뤙기뤙튤', '덱옏증옏증옏뀓옏증옏깐옏몰댈젬닫점액륭증뽐증찌증옏멀울멀뜽옌세뀓밀찌십덱뀓가억땁억삭뀓봬왐욷뀓증쟤증쾯옏덱튿덱쭈왐쪽왠증옌눠증파뀓깁믄흉욜왼왐증덱뀓증뀓덱껍초껵괴쭈점쑬꾼왼증왼암왐증콸봬쁭파쾯댈왐멀밀찌뀓덱뀓증뀓증뀓증뀓뤄왐릴옏젇괴젬괴닫궐쑬증뽐증닫증짱덱옏륵냑젇펀더멀덱공흉증옏증뀓옏뀓옏증붣증멀쌓욜쌓욜뱍욜앵찡숭', '증댈꿔댈렵기댈젬총젬쫑뺀증땐증렙찌팝뀓증섿팡증뱍뵹증릴옏응증륵델은증츨쫑덱씹옏증레욜증셰초뀓초뀓초옏초옏초륵띨러뤙러뤙륵뤙륵뤙륵뤙륵뤙륵옘뤙옘뤙닿뤙쀼쩜잠쩜잠검끋검끋옘닿', '덱뀓증뀓덱뀓덱뀓덱뀓증옏릴옏렵재뵹재옘봬쑬옏력몰짝셰더재째재덱륵증덱뀓덱증덱증덱증뀓증랃덱증재멀덱맹왜재덱쌍탁틴꿀씁릴휘왼멀덱옘초뺨랍엑덱빨냉증뀓증왼옏술빰평멀뀓뻳뵹옘뵹팝옏챋공덱쳔펄줙더앤왐켠증뼌증덱증틴증형재뼐재뜽빵보증근쑬덱캗옥덱형감쿠형증띤증레덱증덱출왐덱켠뀓증돌뜽초륜갇욜몰룬몰쳔중합늡닫왐랭쑬증촌묵쫑쉽술그증숩몰덱증늗꼽왐펭뺨븡급쇧룹괴덱뜽머초증춴점왐증덱근쁭왠뀓왠대큭뀓찌봐뼈뀓증뼈럼릭덱뀓형옏뤌젭씹뀓띤재뀓항닏띨더증술뀓덱옏쟤증륵괴증륵샌륵증', '옏뀓증뀓옏뀓옏증뀓증뀓증뀓증뀓증옏렵의멀쀼뤌늉앤섹초뀓의렇의멀뀓엄흉쫑팝틴덱포증츄뀓증덱질쀼덱질쏭증쏭눠뮤갠뀓겸뀓옏눅옏붣혼텀저입한럼붐파쬬멀뀓윕뀓겸초탁샫뀓증픈겨증놜옏튿증포갠봐옏뀓증붣앤천턷뀓증봉됀증몰옏덜륵뵘륵옏뀓옏뀓옏괴혀괴옏괴옏혀옏', '뀓덛쟤뀓옏흉릴흉릭뤼뻳츄꾼증금렁증넬뽜꾼증재넬뉟뮤넬뵙륵온뀓뭉첼꼽늡륵낑뀓륵릴증되씬뽕씬짠뀓띨계띨꼽믁앤뽜꾼뀓꾼뀓증붕얘붕증숟웡뀓펴땁씁땁증휘엽증퓨증뽕멀증평증씁붕태뀓몰뀓증뀓증뀓옏뀓증뀓옏뀓옏뀓옏젹증쁜증앤몰뀓증뀓앤뀓사몰더앤멀증꼅퓨숭퓨앤커태퉁증몰증퓨초증알륵츄혼츄꾼술증몰꾼몰옏온랃온파뀓륵뀓펑안렫찌증뀓그뀓그증그증뀓그증그증릴받릴증봐침증몰륵숟왐띨증옏증옏그증삭별늑뀓믐뺨뀓받되증온몰재뀓입묻찌왐몰증재욷덱뜽뀓요뭉켇증륵앤퉁술증흐몰옏덱쿠증재뀓증륵눕룹증륵냑꾼퉁뻔묻증뜽증뜽증꾼뀓꾼증합증혼증넬왜증몰뀓온뀓쳐늡륵낑넬뀓찌심릴증칠넬랍꾼앤짝앤랍뀓증뜽꼽룹랍뷰킫찌뀓꾼뭉챋얌릴증릴랍꾼증셰증릴몰뜽몰꾼푿쩨증릴녹숟뭉증욜빱증랍혼증펄적겨몰할랍앤덛꾼증됃몰셰숟앤꾼술꼼퉁꼼챙꾼옏증옏꾼옏꾼몰뻳넬뻳셰옏뎌킵뀓꾼뀓찌뀓꾼뀓그싣뎌몰타온뀓띨껵꽈뀓온츄뀓츄옏뀓츄던뀓증꿈', '낌뀓증요쯥넬쯥증덕증덕띨휘폴흉쯥욕붐짱갱눕욜콘논빠뭥씁뒫별휘틴덱틴요쬐쯥눕붐붕씁덱쇧눕덱쟤흉룹틴앤퉁뀓띨츄챋ㅇ덱증띨겨붜논증재침욜멀브십뀓척증껵졷껵츄휙등위퉁라씁덱뀓붣텀붣덱둭덱뀓녹뺨퉁증퉁츄묻증흉늑츄붣흉칠흉붣꾜팍쾨렏펀렘쩡롸알꾼켠팓룹팓맬뿔팽뿔덛꾼짱멀흉뉵흉꼬덱믹훙눕덱킵금작퉁증퉁츄덱츄낄씬증쉽증꾼증덱걔몰욷덱력겔맬겔슌욷융욷묻꺽벙침로괴눕덱핌덱늘앤찌덱금증쁭혼뷰덱운덱뀓랟륨왐깅덱뀓겨봬씹억판봬덱띨맬뵙쯥룹체츄멀덱훙덱킫금륨냑퉁뀓퉁뀓퉁뀓퉁뀓짣쳑흉겔꾼켈눕띨꾼별펀쯥봬랍꾼덛별갑씁겅삑랟꾼왐꾼퉁덱증왐퉁뀓덱퉁그팍괴쉽괴그늘퉁증위츄증왼랴왼욜띨겅띨증띨덱띨쟤씁뀓붕뺨왐퉁뀓퉁증틈꽈증몰뀓증ㅁ증톱쌤팍톤핌톤몰덱늘더뀓쟤폰퉁뀓윽펴쫑쯥곱뀓몰묻땅츄합찌뀓겨밷셕되뿍덱멀방벙멀셰뭬멀뭬땍온퉁뀓퉁뀓퉁뀓퉁츄퉁덱증뀓덱쟤옏낭옏덱옏뤌껵붕괴껵띨킫덱씁', '뀓옏뀓증뀓덱뀓옏쟤뀓쟤뀓쟤뀓룹쭈옏증뀓덱근떨씬증껵쭈룹증뀓쟤뀓릴옏룹뀓룹뭉뮤줙릴궐펙녹퉁증력너몰룹증옴증왐덱옏덱쭈눙증쏭텔왐증휜흉증츄증걍증헫증옏증슈룹왐룹증뚤증숟증텀쀼옏량랴증왐쭈욜룹몰룹옏증찌궐증껵덱대왐찌룹뚤증숟옏틴점틴왐증왐앤뺀복욜숟뀓꿔왠꿔숟뀓증펴증츄증옏뀓옏뀓증뀓증옏뀓옏뀓뿍긍흉옏젹뎌뻳옏증롭증양께츄증뽠증츄증짼증뀓츄뤄룹증츄뀓뫌녕퀄뀓퉁뀓증뀓증뀓증뀓풉증뀓쀼떨쭈증복몰왐쾯증뀓숟쮜몰욜뜽옌뜽룹뼌뀓스증화뀓며증몰뿔증째온턷증웍증갤멍켱증왐붕증탠짼증욜증칠증몰걍증눙뵈증괭왐증껵녕찌옏뀓왠엔증옏뀓옏봉옏뀓꼼흉릴슫옏뀓옏뀓옏뀓옏뀓옏뀓옏괴룹괴좁괴좁괴룹괴좁룹량룹의룹쭈', '뀓댈릴옏증옘증몰펭녕뀓증뺃증찯숟츄뀓팝앤찌둔릴뀓숟골증줙멀뀓흉왐꿔팝퉁뀓증괭뀓팯증뀓옏증옏멀증해띨증쀼옏뀓옏붣욜깯붣뀓굼', '옏몰옏응흉푿틴앵궐퉁증재앵재붕증충증찌증붕심붕넫증특씬폰씬뀓증천증옏타증겯증눙천뭉씬쿵멀콜옏유숟쌓찌픙옏증팅증쯥왐옏증예폴핻왐귱증멀증응증옏뭉증러증멀옏증굼'] ['웍삐쌈당폭폄쌈롭짼카쌈칟찬업츤돈쌈굴엽쌈뻑폄걱러폄', '댁튼쌈혐뮈댁폭쌈앵년꼬쌈실멛쌈쫙인쌈먹헏짹쩝껍쌈굴튼쌈혐뮈면쌈꽘훌', '댁맹쌈단팡쌈맫실멛쌈싱릅뀐', '댁득감쌈뻑엽뉸쌈덴팯쌈껜쌈굴득꼬쌈죔헐폄감쌈뻑쩝쌈빕맫킴뻐쌈뻑ㄷ쇧뮈쌈스껍팬쌈옹킹뮈쌈첼뻐쌈껙스쌈뮬쐬굴뉸', '댁득봐쌈킴폄쳑쌈둠ㄷ좌립쌈헌츄년쩝쌈끕쎈쌈한쁨뉸', '쑈덜실츤쌈맨럭실쌈람쌈굴댁덥쌈짼멛쌈블당쌈며쌈꺽옏웍쌈규뻳껜쌈묵폭쌈뀓생젼먹쌈꺽옏뉸쌈덜실츤쌈맨럭감뽐쌈뻑톤뻐쌈뻑무쌈람쌈뭐득폄츤웍', '빤중객립뽐뻐쌈껍째먹쌈웍엽냅멛쌈북짼뇩쌈밥쌈굴겍뽐쌈증굳엽립쌈먹굳쌈빱확쌈금덥뽐쌈믕쌈무룹뫼쌈먹쌈뙏슴실뻐쌈빕쯥냅엽쌈웍삐쌈블짼쌈뻑폄', '규뻑덴쌈씬혼쌈뭐엽멛쌈쌕협뿌감쌈윌냅득북뻐쌈감쌈윌뻐쌈껙쌈뮈좌웍쌈뭐뭐헨껍쌈뮈쳑쌈규뻑면쌈냅룰덥쌈실냉쩝쌈왕엽뻐쌈젹쌈협덥셤', '댁튼쌈왱맨쌈굴겍겍쌈쟤땜뻐팯', '댁맹쌈힘앵쌈휩립쌈멷옹쳑쌈믹덜쌈댁섣쌈밥쌈뱀켱카']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 61.412109375,\n",
       " 'eval_wer': 1.0,\n",
       " 'eval_cer': 0.9894227407323989,\n",
       " 'eval_runtime': 2.0345,\n",
       " 'eval_samples_per_second': 4.915,\n",
       " 'eval_steps_per_second': 0.983,\n",
       " 'epoch': 30.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file ./model/custom_processor\\preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"Wav2Vec2Processor\",\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Didn't find file ./model/custom_processor\\added_tokens.json. We won't load it.\n",
      "loading file ./model/custom_processor\\vocab.json\n",
      "loading file ./model/custom_processor\\tokenizer_config.json\n",
      "loading file None\n",
      "loading file ./model/custom_processor\\special_tokens_map.json\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['윔', '읗', '처', '옏', '궤', '처', '똠', '깔', '결', '처', '쏘', '홉', '눤']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer\n",
    "from utils.DataCollatorCTCWithPadding import DataCollatorCTCWithPadding\n",
    "import IPython.display as ipd\n",
    "import nlptutti as metrics\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "# from utils.compute_metrics import compute_metrics\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./model/custom_processor\")\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained('./model/custom_model',\n",
    "#                                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#                                        ignore_mismatched_sizes=True)\n",
    "\n",
    "processor.batch_decode(subset_test['labels'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasets_df = pd.read_csv('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/data/final_dataset_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with processor.as_target_processor():\n",
    "    label = processor(list(datasets_df['g2p_text'][:2])).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저는 베트나메서 와써요 항국 무놔를 조아애서 항구게 오게 돼써요',\n",
       " '저는 베트나메서 와써요 항구게 온 이유는 어릴 때부터 저는 항국 드라마를 마니 보고 항구게 살고 시퍼써요']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['갸낼처때벼변처턷뉼깯처뱅롼쿵술꾼처둔설처흉변쭝릳변',\n",
       " '윔됃처엥돋윔벼처밤턴말처깔결처룩ㄷ처쩌뭄퉈열합처둔됃처엥돋빋처붜절',\n",
       " '윔읗처옏궤처똠깔결처쏘홉눤',\n",
       " '윔탐덥처흉설질처찜돕처랴처둔탐말처욈꼼변덥처흉열처근똠뻘먕처흉꿘봬돋처꾀합뢰처셕쩡돋처환먕처핃꾀처놜텐둔질',\n",
       " '윔탐멛처뻘변헫처톤꿘껴팅처랃믹턴열처뷔잼처상켝질',\n",
       " '삽삼깔술처밀먿깔처뵈처둔윔썩처뉼결처면때처뿍처빙럭갸처쌰쓔랴처반벼처퉏늡뽣쩌처빙럭질처삼깔술처밀먿덥펭처흉컵먕처흉몀처뵈처픔탐변술갸',\n",
       " '춴맬토팅펭먕처합임쩌처갸설김결처홤뉼빧처험처둔잘펭처귄빔설팅처쩌빔처텅렌처씬썩펭처람처몀꿕총처쩌처히쭫깔먕처근쿠김설처갸낼처면뉼처흉변',\n",
       " '쌰흉찜처꽈붱처픔설결처롤틀냅덥처웡김탐홤먕처덥처웡먕처핃처돋껴갸처픔플합처돋헫처쌰흉빋처김속썩처깔엗열처젬설먕처뙏처틀썩각',\n",
       " '윔됃처능밀처둔잘처돌구먕돕',\n",
       " '윔읗처츨밤처문팅처탣셕헫처뼝삼처윔딱처험처쀼갭깯']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(subset_test['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, Wav2Vec2ProcessorWithLM,Wav2Vec2Processor\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from jamo import h2j, j2hcj\n",
    "from sklearn.model_selection import train_test_split\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/model/feature_extractor_conformer')\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/model/feature_extractor_tokenizer')\n",
    "\n",
    "# beamsearch_decoder = build_ctcdecoder(\n",
    "#     labels=list(tokenizer.encoder.keys()),\n",
    "#     kenlm_model_path=None,\n",
    "# )\n",
    "\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403abc02212346c5a876bc01651eed1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fe1f319e814806a922afe80d1593ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62f3a3d93c04649a7b64ed09804c644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('./data/pre_datasets')\n",
    "subset_size = 10\n",
    "subset_train = data['train'].select([i for i in range(subset_size)])\n",
    "subset_train\n",
    "\n",
    "test_size = 10\n",
    "subset_test = data['test'].select([i for i in range(test_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datasets_df = pd.read_csv('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/data/final_dataset_df.csv', encoding='utf-8')\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    print(processor(list(datasets_df['text'][:10].map(lambda x : j2hcj(h2j(x))).str.replace(' ', '|'))).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object hangul_to_jamo.<locals>.<genexpr> at 0x000001F105C59040>\n",
      "[[5], [6], [5], [42], [4], [5], [6], [5], [6]]\n"
     ]
    }
   ],
   "source": [
    "import jamo\n",
    "\n",
    "print(jamo.hangul_to_jamo('아야 어여'))\n",
    "with processor.as_target_processor():\n",
    "    print(processor(list(lt)).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['앱들 보면 자동 업데이트가 되는 게 있어',\n",
       " '백만원짜리 휴대폰을 폼으로 가지고 다니는구만',\n",
       " '여자들이 디퓨저를 엄청 좋아하잖아요',\n",
       " '그럼 한번 타 보고 결정할 수 있을까요 오늘 구입하고 싶습니다',\n",
       " '내 친구 중 한 명은 주말에 쉴 때마다 직장 동료들이랑 같이 산으로 자전거를 타러 가는 걸 즐긴대',\n",
       " '해외에서는 다양하고 푸짐하게 제공되는 한국의 반찬 문화를 정말 독특하게 생각하여 신기하기도 한다',\n",
       " '만약에 코로나가 없어진다고 하면 가장 가고 싶은 곳은 어<unk> 중국 고향입니다 어<unk> 거기 가서 부모님과 같이 식사를 하거나 산책을 하고 싶어요',\n",
       " '심지어 인천공항과 김포공항도 비행기를 타고 고향으로 향하는 사람들과 여행을 떠나려는 사람들로 붐빈다',\n",
       " '함께 다문화 강의를 하는 선생님의 집으로 자주 갑니다 거기서 쌍방향으로 인터넷으로 강의를 하고 있습니다',\n",
       " '살면서 가장 부끄러웠던 경험은 달리기를 하다가 운동장에 쓰러져 세 사람이 같이 들어주어도 나를 들어주지 못한 것이다']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(list(subset_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, AutoFeatureExtractor, AutoTokenizer, AutoModelForCTC\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer\n",
    "from utils.DataCollatorCTCWithPadding import DataCollatorCTCWithPadding\n",
    "import IPython.display as ipd\n",
    "import nlptutti as metrics\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import os\n",
    "# from utils.compute_metrics import compute_metrics\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/model/feature_extractor_conformer')\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/model/feature_extractor_tokenizer')\n",
    "\n",
    "# beamsearch_decoder = build_ctcdecoder(\n",
    "#     labels=list(tokenizer.encoder.keys()),\n",
    "#     kenlm_model_path=None,\n",
    "# )\n",
    "\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained('42MARU/ko-spelling-wav2vec2-conformer-del-1s',\n",
    "                                        ctc_loss_reduction=\"mean\", \n",
    "                                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                                        )\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "data = load_dataset('./data/pre_datasets')\n",
    "subset_size = 10\n",
    "subset_train = data['train'].select([i for i in range(subset_size)])\n",
    "subset_train\n",
    "\n",
    "test_size = 10\n",
    "subset_test = data['test'].select([i for i in range(test_size)])\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./model/trained_model',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=4,\n",
    "  gradient_accumulation_steps=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=3,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500, \n",
    "  save_total_limit=1,\n",
    "  push_to_hub=False,\n",
    "  dataloader_pin_memory=False,\n",
    "  logging_dir='./logs',\n",
    "  report_to = \"tensorboard\",\n",
    "  load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "import nlptutti as metrics\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wer_metric = 0\n",
    "    cer_metric = 0\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    # print(pred_str, label_str)\n",
    "    \n",
    "    for i in range(len(pred_str)):\n",
    "        # preds = pred_str[i].replace(\" \", \"\")\n",
    "        # labels = label_str[i].replace(\" \", \"\")\n",
    "        wer = metrics.get_wer(pred_str[i], label_str[i])['wer']\n",
    "        cer = metrics.get_cer(pred_str[i], label_str[i])['cer']\n",
    "        wer_metric += wer\n",
    "        cer_metric += cer\n",
    "        \n",
    "    wer_metric = wer_metric/len(pred_str)\n",
    "    cer_metric = cer_metric/len(pred_str)\n",
    "    \n",
    "    return {\"wer\": wer_metric, \"cer\": cer_metric}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=subset_train,\n",
    "    eval_dataset=subset_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16b936e3e0445fe952da4db5ea34816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['네 감사합니다 그러면 혹시 한 가지만 더 여쭤봐도 될까요 지금 클릭회안이 4급 과목은 자리가가 찾다고 하네요ᄋ 아ᄋ아ᄋ', '원래는 셋체널 광안리 해수욕장을 보고 뼝백섬에 가려고 했는데 그날도 비가 온다고 해서 내체널 가기로 했어요 서', '진도는 남죽에 있어서 서울에서 가려면 너수를 오래 타야 해요 딱 1년에 한 번 바닷길이 열려서 다다 사이로 길이 만들어져요서야ᄋ', '한국에서 찜질방에 갚은 적이 있어요 찜질방에서 주로 곡원으로 몸에 이완시켜서 희도 효과도 있어요', '돼지고기가 저 고향에 있는 고기보다 참 맛있다고 생각합니다 이유는 잘 모르겠지만 그 돼지고기 삼겹살 같은 거 브럽고 그 맛도 있고 뭐가 확실히 차이가 있다고 생각합니다', '아침을 많이 먹어서 그런지 또다시 배가 아프고 빵빵해지기 시작해서 급하게 지자철에서 내렸습니다ᅥ서', '귀농을 원하는 사람들 중에 그런 보람을 느끼고 싶어 하는 사람들이 많더라고', '친구들과 함께 높은 곳에서 신흡탕 속으로 풍떵 뛰어내려는데 머리부터 발끝까지 찐흐끄롯 뜃 덮혔어요', '한 벌에 25만원인데 디자인 너무 마음에 들어서 빨간색이랑 주광색 본홍세 가지 다 사버렸어', '네 그러면 언룬 엽서를 만들고 가서 오체국 가서 확인해 봐야갔어요 덕분에 선물 고민이 해결했어요'] ['네 감사합니다 그러면 혹시 한 가지만 더 여쭤 봐도 될까요 지금 클릭해바니 사 급 과목은 자리가 다 찼다고 하네요', '원래는 셋째 날 광안리 해수욕장을 보고 동백섬에 가려고 했는데 그날도 비가 온다고 해서 넷째 날 가기로 했어요', '진도는 남쪽에 있어서 서울에서 가려면 버스를 오래 타야 해요 딱 일 년에 한 번 바닷길이 열려서 바다 사이로 길이 만들어져요', '한국에서 찜질방에 가본 적이 있어요 찜질방에서 주로 그 원리로 몸이 이완 시켜서 해독 효과도 있어요', '돼지고기가 저 고향에 있는 고기보다 참 맛있다고 생각합니다 이유는 잘 모르겠지만 그<unk> 돼지고기 삼겹살 같은 거 부드럽고 그<unk> 맛도 있고 뭐가 확실히 차이가 있다고 생각합니다', '아침을 많이 먹어서 그런지 또다시 배가 아프고 빵빵해지기 시작해서 급하게 지하철에서 내렸습니다', '귀농을 원하는 사람들 중에 그런 보람을 느끼고 싶어 하는 사람들이 많더라고', '친구들과 함께 높은 곳에서 진흙탕 속으로 풍덩 뛰어내렸는데 머리부터 발끝까지 진흙으로 뒤덮였어요', '한 벌에 이십오만 원인데 디자인 너무 마음에 들어서 빨간색이랑 주황색 분홍색까지 다 사 버렸어', '네 그러면 얼른 엽서를 만들고 가서 우체국 가서 확인해 봐야겠어요 덕분에 선물 고민을 해결했어요']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3126583695411682,\n",
       " 'eval_wer': 0.31076036244457295,\n",
       " 'eval_cer': 0.08452066622101226,\n",
       " 'eval_runtime': 1.1505,\n",
       " 'eval_samples_per_second': 8.692,\n",
       " 'eval_steps_per_second': 1.738,\n",
       " 'epoch': 30.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09b4ed94193438ab25bdac9b26e7edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281bc4dedadf406faeca4328c00400c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "# from pyctcdecode import build_ctcdecoder\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForCTC,\n",
    "    AutoTokenizer,\n",
    "    Wav2Vec2ProcessorWithLM,\n",
    "    Wav2Vec2Processor\n",
    ")\n",
    "import unicodedata\n",
    "from transformers.pipelines import AutomaticSpeechRecognitionPipeline\n",
    "import torch, gc\n",
    "\n",
    "# audio_path = \"C:/Users/user/Downloads/131.인공지능 학습을 위한 외국인 한국어 발화 음성 데이터/01.데이터_new_20220719/2.Validation/원천데이터/VS_4. 중국어/5. 한국문화II/CN50QA286_CN0476_20211014.wav\"\n",
    "\n",
    "# 모델과 토크나이저, 예측을 위한 각 모듈들을 불러옵니다.\n",
    "model = AutoModelForCTC.from_pretrained(\"42MARU/ko-spelling-wav2vec2-conformer-del-1s\").to('cuda')\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"42MARU/ko-spelling-wav2vec2-conformer-del-1s\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"42MARU/ko-spelling-wav2vec2-conformer-del-1s\")\n",
    "# beamsearch_decoder = build_ctcdecoder(\n",
    "#     labels=list(tokenizer.encoder.keys()),\n",
    "#     kenlm_model_path=None,\n",
    "# )\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer\n",
    ")\n",
    "data = load_dataset('./data/pre_datasets')\n",
    "subset_size = 10\n",
    "subset_train = data['train'].select([i for i in range(subset_size)])\n",
    "subset_train\n",
    "\n",
    "test_size = 10\n",
    "subset_test = data['test'].select([i for i in range(test_size)])\n",
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "  return batch\n",
    "\n",
    "results = subset_test.map(map_to_result, remove_columns=subset_test.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlptutti as metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "unicodedata.normalize(\"NFC\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cer': 0.2608695652173913,\n",
       " 'substitutions': 12,\n",
       " 'deletions': 0,\n",
       " 'insertions': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = unicodedata.normalize(\"NFC\", results['pred_str'][0])\n",
    "kk = unicodedata.normalize(\"NFC\", results['text'][0])\n",
    "\n",
    "metrics.get_cer(k, kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cer': 0.17886178861788618,\n",
       " 'substitutions': 12,\n",
       " 'deletions': 1,\n",
       " 'insertions': 9}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.get_cer(results['pred_str'][0], results['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한국의 최초 신식 결혼식은 천팔백구십년 정동 교회에서 신도인 박신실과 강신성의 결혼식이었다는 기록이 있다',\n",
       " '잉어의 정소가 텅 비어 있습니다',\n",
       " '저는 떡볶이 먹고 싶은데 그럼 모두 다 시켜서 나눠 먹어요 우선 피자와 치킨 둘 다 피<unk> 파는 곳을 찾아볼까요',\n",
       " '자갈치 시장에 매운탕이 맛있는 가게가 있다고 해서 저희는 회와 매운탕을 먹기로 결정했습니다',\n",
       " '이 가방 말씀하시는 거군요 원래 최대 무게는 이십삼 킬로그램이지만 수하물을 많이 안 보내시니 괜찮을 것 같습니다',\n",
       " '콤비네이션 피자와 하와이안 피자 반반으로 해 주시고요 치킨은 양념 반 프라이드 반으로 주문할게요',\n",
       " '먼저 미역국은 씻은 다음에 물에 넣어서 삼십 분 동안 불리고요 소고기는 먹기 좋게 한봉 크기로 잘라 줘요',\n",
       " '그럴 경우 가족 관계 증명서가 필요합니다 가족 관계 증명서 가지고 오시면 저희가 예약할 수 있도록 도와드리겠습니다',\n",
       " '그런데 익숙해지지 않으면 귀농은 안하는 게 나아',\n",
       " '그<unk> 한국 문화를 이해할 수 있도록 한국 역사를 공부해야 한다고 생각합니다 왜냐면 그<unk> 뿌리는 알 수 있기 때문에 그<unk> 역사를 배우는 게 추천합니다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한국에 죄조 신식 결혼식 은 21890년 전동 교회에서 신도인 박 신실과 강신성에 결혼식이었다는 길억이 있다',\n",
       " '잉어의 정소가 텅비어있습니다',\n",
       " '저는 떡볶이 먹고 싶은데 그런 모두 다 시켜서 라면 먹어요 우선 피자와 치킨 둘 다 피 파는 곳을 찾아볼까요',\n",
       " '자갈치 시장에 매온당이 맛있는 가게가 있다고 해소 겨휘놈 호애과 매온당물 먹기로 결청했습니다',\n",
       " '이 가방 말씀하시는거군요 원래 최다 모기는 2 73규로 부랩이지만 소화물은 많이 안 포내지니 괜찮을 것 같습니다',\n",
       " '콘비네이션 빗자와 하와이ᄋ안 비자 반반으로 해주시고요 치킨은 양념반 프라이드 반으로 주문할게요',\n",
       " '먼저 미어극은 싯은 다음에 문을 넣어서 30분 동안 부릴 거에요 소고기는 먹기 적게 한 분 크기로 짤라줘요',\n",
       " '그럴 경우 가족 관계 등명소가 필요합니다 가족 관계 등명소 가지고 오시면 저희가 예약할 수 있도록 도와드리겠습니다',\n",
       " '그런데 익숙해지지 않으면 기능은 안 하는 게 나아',\n",
       " 'ᄋ 그 한국 문화를 이해할 수 있도록 한국 역사를 공부해야 한다고 생각합니다 왜냐면 그 뿌이는 알 수 있기 때문에 그 역사를 배우는 게 추천합니다']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['pred_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "datasets_df = pd.read_csv('C:/Users/jjw28/OneDrive/바탕 화면/wav2vec2/data/final_dataset_df.csv', encoding='utf-8')   \n",
    "_, datasets_df = train_test_split(datasets_df, stratify=datasets_df['country'], test_size = 0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "r,_ = librosa.load(datasets_df['audio_path'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00384647, 0.00384647, 0.00384647, ..., 0.00384647, 0.00384647,\n",
       "       0.00384647], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(r,sampling_rate=16000).input_values[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
